{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import all_estimators\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import pandas as pd\n",
    "from mlflow.models.signature import infer_signature\n",
    "import mlflow\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather('../data/carteira_total_with_quality_score.feather')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_utilizado_relativo'] = (np.mean(df.total_utilizado) - df.total_utilizado) / df.total_utilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_utilizado_relativo'].fillna(df['total_utilizado_relativo'].max(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nao_utilizado_total'] = df['total_contratado'] - df['total_utilizado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"pf_pj\", \"total_utilizado_relativo\", \"contratado_ofertas_simples\", \"utilizado_ofertas_simples\", \"leads_form\",\n",
    " \"equipe\", \"utilizado_destaque\", \"valor_mensal\", \"quantidade_mes\", \"status_pagamento\", \"regiao\", \"oficina\", \"tipo_de_plano\", \"frequencia_de_faturamento\", 'churn']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['contratado_ofertas_simples', 'utilizado_ofertas_simples',\n",
    "                    'leads_form', 'utilizado_destaque', 'valor_mensal', 'quantidade_mes']\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    ")\n",
    "\n",
    "categorical_features = ['pf_pj', 'equipe',\n",
    "                        'regiao', 'oficina', 'tipo_de_plano', 'frequencia_de_faturamento']\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[(\"onehot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "           (\"imputer\", SimpleImputer(strategy=\"constant\"))]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"scaler2\", RobustScaler(with_centering=False)),\n",
    "        ('smote', RandomUnderSampler(sampling_strategy='majority', random_state=42))\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['churn'], axis=1).copy()\n",
    "y = df['churn'].astype(int).copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAndSave(pipeline, X_test, y_test, y_pred):\n",
    "    mlflow.set_experiment('Churn Prediction')\n",
    "    signature = infer_signature(X_test, y_pred)\n",
    "    mlflow.sklearn.log_model(pipeline, 'model_pipeline', signature=signature)\n",
    "    params = pipeline.named_steps[\"classifier\"].get_params()\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    plot_confusion_matrix(pipeline, X_test, y_test, display_labels=[\n",
    "        \"positivo\", \"negativo\"], values_format=\"d\")\n",
    "\n",
    "    plt.savefig(\"mlruns/confusion_matrix_.png\")\n",
    "    mlflow.log_artifact(\"mlruns/confusion_matrix_.png\")\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    mlflow.log_metric(\"metrics\", classification_report(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = all_estimators(type_filter='classifier')\n",
    "\n",
    "all_class = []\n",
    "for name, ClassificationClass in estimators:\n",
    "    try:\n",
    "        clas = ClassificationClass()\n",
    "        all_class.append(clas)\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/06/02 23:30:16 INFO mlflow.tracking.fluent: Experiment with name 'Churn Prediction' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88     44628\n",
      "           1       0.29      0.82      0.43      4605\n",
      "\n",
      "    accuracy                           0.80     49233\n",
      "   macro avg       0.63      0.81      0.65     49233\n",
      "weighted avg       0.91      0.80      0.83     49233\n",
      "\n",
      "BaggingClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.82      0.89     44628\n",
      "           1       0.30      0.77      0.44      4605\n",
      "\n",
      "    accuracy                           0.81     49233\n",
      "   macro avg       0.64      0.80      0.66     49233\n",
      "weighted avg       0.91      0.81      0.85     49233\n",
      "\n",
      "BernoulliNB()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.71      0.81     44628\n",
      "           1       0.20      0.69      0.31      4605\n",
      "\n",
      "    accuracy                           0.71     49233\n",
      "   macro avg       0.58      0.70      0.56     49233\n",
      "weighted avg       0.89      0.71      0.77     49233\n",
      "\n",
      "CalibratedClassifierCV()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.78      0.86     44628\n",
      "           1       0.27      0.79      0.40      4605\n",
      "\n",
      "    accuracy                           0.78     49233\n",
      "   macro avg       0.62      0.78      0.63     49233\n",
      "weighted avg       0.91      0.78      0.82     49233\n",
      "\n",
      "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n",
      "ComplementNB()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.53      0.69     44628\n",
      "           1       0.16      0.83      0.26      4605\n",
      "\n",
      "    accuracy                           0.56     49233\n",
      "   macro avg       0.56      0.68      0.48     49233\n",
      "weighted avg       0.89      0.56      0.65     49233\n",
      "\n",
      "DecisionTreeClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.77      0.86     44628\n",
      "           1       0.25      0.76      0.38      4605\n",
      "\n",
      "    accuracy                           0.77     49233\n",
      "   macro avg       0.61      0.77      0.62     49233\n",
      "weighted avg       0.90      0.77      0.81     49233\n",
      "\n",
      "DummyClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95     44628\n",
      "           1       0.00      0.00      0.00      4605\n",
      "\n",
      "    accuracy                           0.91     49233\n",
      "   macro avg       0.45      0.50      0.48     49233\n",
      "weighted avg       0.82      0.91      0.86     49233\n",
      "\n",
      "ExtraTreeClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.75      0.84     44628\n",
      "           1       0.23      0.73      0.35      4605\n",
      "\n",
      "    accuracy                           0.75     49233\n",
      "   macro avg       0.60      0.74      0.60     49233\n",
      "weighted avg       0.90      0.75      0.80     49233\n",
      "\n",
      "ExtraTreesClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.78      0.87     44628\n",
      "           1       0.27      0.79      0.41      4605\n",
      "\n",
      "    accuracy                           0.78     49233\n",
      "   macro avg       0.62      0.79      0.64     49233\n",
      "weighted avg       0.91      0.78      0.83     49233\n",
      "\n",
      "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n",
      "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n",
      "GradientBoostingClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.80      0.88     44628\n",
      "           1       0.30      0.83      0.45      4605\n",
      "\n",
      "    accuracy                           0.81     49233\n",
      "   macro avg       0.64      0.82      0.66     49233\n",
      "weighted avg       0.92      0.81      0.84     49233\n",
      "\n",
      "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n",
      "KNeighborsClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.79      0.87     44628\n",
      "           1       0.27      0.77      0.40      4605\n",
      "\n",
      "    accuracy                           0.78     49233\n",
      "   macro avg       0.62      0.78      0.63     49233\n",
      "weighted avg       0.90      0.78      0.82     49233\n",
      "\n",
      "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n",
      "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n",
      "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n",
      "LinearSVC()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.77      0.86     44628\n",
      "           1       0.27      0.80      0.40      4605\n",
      "\n",
      "    accuracy                           0.78     49233\n",
      "   macro avg       0.62      0.78      0.63     49233\n",
      "weighted avg       0.91      0.78      0.82     49233\n",
      "\n",
      "LogisticRegression()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.78      0.86     44628\n",
      "           1       0.27      0.79      0.40      4605\n",
      "\n",
      "    accuracy                           0.78     49233\n",
      "   macro avg       0.62      0.79      0.63     49233\n",
      "weighted avg       0.91      0.78      0.82     49233\n",
      "\n",
      "LogisticRegressionCV()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.78      0.86     44628\n",
      "           1       0.27      0.79      0.40      4605\n",
      "\n",
      "    accuracy                           0.78     49233\n",
      "   macro avg       0.62      0.79      0.63     49233\n",
      "weighted avg       0.91      0.78      0.82     49233\n",
      "\n",
      "MLPClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.80      0.88     44628\n",
      "           1       0.30      0.82      0.44      4605\n",
      "\n",
      "    accuracy                           0.80     49233\n",
      "   macro avg       0.64      0.81      0.66     49233\n",
      "weighted avg       0.91      0.80      0.84     49233\n",
      "\n",
      "MultinomialNB()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.53      0.69     44628\n",
      "           1       0.16      0.83      0.26      4605\n",
      "\n",
      "    accuracy                           0.56     49233\n",
      "   macro avg       0.56      0.68      0.48     49233\n",
      "weighted avg       0.89      0.56      0.65     49233\n",
      "\n",
      "NearestCentroid()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.43      0.60     44628\n",
      "           1       0.14      0.88      0.24      4605\n",
      "\n",
      "    accuracy                           0.47     49233\n",
      "   macro avg       0.55      0.65      0.42     49233\n",
      "weighted avg       0.89      0.47      0.56     49233\n",
      "\n",
      "NuSVC()\n"
     ]
    }
   ],
   "source": [
    "for i in all_class:\n",
    "    pipeline.steps.append(('classifier', i))\n",
    "\n",
    "    try:\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        print(i)\n",
    "        printAndSave(pipeline, X_test, y_test, y_pred)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    pipeline.steps.remove(('classifier', i))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60683c818ea6f768f811ad5f90736ebd9ed15f5ee0114937570dcd60c186bf40"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
